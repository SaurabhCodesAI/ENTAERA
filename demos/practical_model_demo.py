#!/usr/bin/env python3
"""
üéØ PRACTICAL MODEL USAGE EXAMPLES
=================================
Show exactly how to use your AI models in code
"""

import sys
import os
sys.path.append('src')

def demo_model_usage():
    """Demonstrate practical model usage"""
    
    import os
    print("ü§ñ PRACTICAL AI MODEL USAGE DEMO")
    print("=" * 50)
    
    print("üìç Working Directory:", os.getcwd())
    print("üéØ This demo shows how your models actually work in practice")
    
    # Example 1: Semantic Search (Embedding Model)
    print(f"\n1Ô∏è‚É£ SEMANTIC SEARCH WITH EMBEDDINGS")
    print("-" * 40)
    
    try:
        from entaera.core.semantic_search import SemanticSearchEngine, SentenceTransformerProvider
        
        print("‚úÖ Loading sentence-transformer embedding model...")
        provider = SentenceTransformerProvider()
        search_engine = SemanticSearchEngine(provider)
        
        # Add some sample content
        documents = [
            "Python is a programming language",
            "Machine learning uses algorithms",
            "Web development with HTML and CSS",
            "Database management with SQL"
        ]
        
        print(f"üìù Adding {len(documents)} documents to search index...")
        for i, doc in enumerate(documents):
            search_engine.add_content(f"doc-{i}", doc, {"type": "sample"})
        
        # Perform search
        query = "programming languages"
        print(f"\nüîç Searching for: '{query}'")
        results = search_engine.search(query, top_k=3)
        
        for i, result in enumerate(results, 1):
            print(f"   {i}. Score: {result.similarity_score:.3f} | {result.content_id}")
            
        print("‚úÖ Embedding model working perfectly!")
        
    except Exception as e:
        print(f"‚ùå Semantic search error: {e}")
    
    # Example 2: Configuration and Model Paths
    print(f"\n2Ô∏è‚É£ MODEL CONFIGURATION")
    print("-" * 40)
    
    try:
        from entaera.core.config import ApplicationSettings
        settings = ApplicationSettings()
        
        print("üìã Current model configuration:")
        
        # Check environment variables
        import os
        model_vars = {
            'LOCAL_AI_ENABLED': os.getenv('LOCAL_AI_ENABLED', 'Not set'),
            'CODE_MODEL_PATH': os.getenv('CODE_MODEL_PATH', 'Not set'),
            'CHAT_MODEL_PATH': os.getenv('CHAT_MODEL_PATH', 'Not set'),
            'EMBEDDING_MODEL': os.getenv('EMBEDDING_MODEL', 'Not set'),
            'CUDA_ENABLED': os.getenv('CUDA_ENABLED', 'Not set'),
        }
        
        for var, value in model_vars.items():
            print(f"   {var}: {value}")
            
        # Check if model files exist
        from pathlib import Path
        print(f"\nüìÅ Model file status:")
        
        model_paths = [
            os.getenv('CODE_MODEL_PATH', ''),
            os.getenv('CHAT_MODEL_PATH', '')
        ]
        
        for path in model_paths:
            if path:
                file_path = Path(path)
                if file_path.exists():
                    size_gb = file_path.stat().st_size / (1024**3)
                    print(f"   ‚úÖ {file_path.name}: {size_gb:.1f}GB")
                else:
                    print(f"   ‚ùå {path}: Not found")
        
    except Exception as e:
        print(f"‚ùå Configuration error: {e}")
    
    # Example 3: How Models Would Be Used in Chat
    print(f"\n3Ô∏è‚É£ HOW MODELS INTEGRATE WITH CONVERSATIONS")
    print("-" * 40)
    
    try:
        from entaera.core.conversation import ConversationManager, Conversation
        
        print("üí¨ Creating conversation manager...")
        conv_manager = ConversationManager()
        
        print("üìù Creating a sample conversation...")
        conversation = conv_manager.create_conversation("AI Chat Demo")
        
        # Add system message
        conversation.add_system_message("You are a helpful AI assistant.")
        
        # Add user message
        conversation.add_user_message("Hello! Can you help me with Python?")
        
        print(f"üìä Conversation statistics:")
        stats = conversation.get_statistics()
        print(f"   Messages: {stats.total_messages}")
        print(f"   Tokens: {stats.total_tokens}")
        print(f"   Context utilization: {stats.context_utilization:.1%}")
        
        print("üí° In a real chat app, this is where you'd:")
        print("   1. Take the conversation context")
        print("   2. Send it to Llama 3.1 8B model")
        print("   3. Get AI response")
        print("   4. Add response back to conversation")
        
        print("‚úÖ Conversation system ready for AI integration!")
        
    except Exception as e:
        print(f"‚ùå Conversation error: {e}")
    
    # Example 4: Model Loading Process
    print(f"\n4Ô∏è‚É£ HOW MODELS ARE ACTUALLY LOADED")
    print("-" * 40)
    
    print("üîß Model loading process:")
    print("   1. Read .env.local_ai configuration")
    print("   2. Initialize PyTorch with CUDA settings")
    print("   3. Load GGUF file into memory")
    print("   4. Allocate GPU layers (35 for RTX 4050)")
    print("   5. Set context window and generation params")
    print("   6. Ready for inference!")
    
    print(f"\nüíª Your hardware setup:")
    try:
        import torch
        print(f"   PyTorch: {torch.__version__}")
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f"   GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        else:
            print("   GPU: CUDA not available, using CPU")
    except:
        print("   PyTorch: Not available")
    
    # Example 5: Performance Characteristics
    print(f"\n5Ô∏è‚É£ PERFORMANCE CHARACTERISTICS")
    print("-" * 40)
    
    print("‚ö° Model performance on your RTX 4050:")
    print("   üìä Embedding Model (sentence-transformers):")
    print("      ‚Ä¢ Speed: ~100-150 docs/second")
    print("      ‚Ä¢ Memory: ~200MB")
    print("      ‚Ä¢ Latency: <50ms per query")
    
    print("   üß† Llama 3.1 8B (Chat Model):")
    print("      ‚Ä¢ Speed: ~10-20 tokens/second")
    print("      ‚Ä¢ Memory: ~4.6GB")
    print("      ‚Ä¢ Context: 4096 tokens")
    
    print("   üíª CodeLlama 7B (Code Model):")
    print("      ‚Ä¢ Speed: ~15-25 tokens/second")
    print("      ‚Ä¢ Memory: ~3.8GB")
    print("      ‚Ä¢ Specialization: Programming tasks")
    
    print(f"\nüéØ READY TO BUILD:")
    print("Your models are configured and ready!")
    print("You can now build:")
    print("‚Ä¢ AI chat applications")
    print("‚Ä¢ Code generation tools")
    print("‚Ä¢ Semantic search systems")
    print("‚Ä¢ Multi-agent workflows")
    
    print(f"\n" + "="*50)
    print("üöÄ Your AI models are production-ready!")

if __name__ == "__main__":
    demo_model_usage()