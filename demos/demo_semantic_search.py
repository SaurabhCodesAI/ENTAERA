#!/usr/bin/env python3
"""
Day 4 Kata 4.1 - Semantic Search Demonstration
Comprehensive demo of semantic search capabilities including vector embeddings, 
similarity search, conversation integration, and advanced features.
"""

import asyncio
import tempfile
from datetime import datetime, timezone, timedelta
from pathlib import Path
import time

import sys
sys.path.append('src')

from entaera.core.semantic_search import (
    SemanticSearchEngine, SentenceTransformerProvider, SearchFilter,
    SimilarityAlgorithm, EmbeddingProvider
)
from entaera.core.conversation import (
    ConversationManager, Conversation, Message, MessageRole, MessageType,
    ConversationStatus
)


def print_header(title: str, char: str = "=", width: int = 70):
    """Print a formatted header."""
    print(f"\n{char * width}")
    print(f"ğŸš€ {title}")
    print(f"{char * width}")


def print_subheader(title: str, char: str = "-", width: int = 50):
    """Print a formatted subheader.""" 
    print(f"\n{title}")
    print(f"{char * width}")


async def demo_basic_embedding_and_search():
    """Demonstrate basic embedding generation and similarity search."""
    print_subheader("ğŸ§  Basic Embedding & Similarity Search Demo")
    
    # Initialize semantic search engine
    temp_dir = Path(tempfile.mkdtemp())
    provider = SentenceTransformerProvider("all-MiniLM-L6-v2")
    engine = SemanticSearchEngine(provider=provider, cache_dir=temp_dir)
    
    print("âœ… Initialized semantic search engine")
    print(f"ğŸ“ Cache directory: {temp_dir}")
    
    # Add sample content
    contents = [
        "Python is a powerful programming language used for web development",
        "Machine learning algorithms can analyze large datasets efficiently", 
        "JavaScript enables interactive web applications and user interfaces",
        "Data science involves statistical analysis and predictive modeling",
        "Web scraping extracts data from websites using automated tools",
        "Neural networks are inspired by biological brain structures",
        "Database management systems store and organize business data",
        "Cloud computing provides scalable infrastructure for applications"
    ]
    
    print(f"\nğŸ“ Adding {len(contents)} content items to search index...")
    start_time = time.time()
    
    for i, content in enumerate(contents):
        await engine.add_content(
            content=content,
            source_id=f"content-{i}",
            source_type="article",
            tags=["demo", "programming", "technology"]
        )
        
    indexing_time = time.time() - start_time
    print(f"â±ï¸  Indexed {len(contents)} items in {indexing_time:.2f} seconds")
    
    # Perform semantic searches
    queries = [
        "Python programming language",
        "machine learning and AI",
        "web development technologies", 
        "data analysis and statistics",
        "cloud infrastructure"
    ]
    
    print(f"\nğŸ” Performing {len(queries)} semantic searches...")
    
    for query in queries:
        print(f"\nğŸ” Query: '{query}'")
        start_time = time.time()
        
        results = await engine.search(query)
        search_time = time.time() - start_time
        
        print(f"âš¡ Found {len(results)} results in {search_time:.3f}s")
        
        for i, result in enumerate(results[:3]):  # Show top 3
            # Get the embedding ID from the search result
            embedding_id = result.id
            if embedding_id in engine.index.embeddings:
                embedding = engine.index.embeddings[embedding_id]
                content_preview = embedding.metadata.source_id
            else:
                content_preview = result.source_id or result.content[:50]
            score = result.similarity_score
            print(f"   {i+1}. Score: {score:.3f} | {content_preview}")
            
    # Display engine statistics
    stats = engine.get_stats()
    print(f"\nğŸ“Š Search Engine Statistics:")
    print(f"   ğŸ”¢ Total embeddings: {stats['index_stats']['total_embeddings']}")
    print(f"   ğŸ“ Vector dimensions: {stats['index_stats']['dimensions']}")
    print(f"   ğŸ¤– Model: {stats['model_info']['model_name']}")
    print(f"   ğŸ’¾ Memory usage: {stats['index_stats']['memory_usage_mb']:.2f} MB")
    print(f"   ğŸ—„ï¸ Cached embeddings: {stats['cache_stats']['cached_embeddings']}")
    print(f"   ğŸ“ˆ Cache utilization: {stats['cache_stats']['utilization']:.6f}")
    
    return engine, temp_dir


async def demo_similarity_algorithms():
    """Demonstrate different similarity algorithms."""
    print_subheader("âš–ï¸ Similarity Algorithm Comparison Demo")
    
    # Create test data with clear semantic relationships
    temp_dir = Path(tempfile.mkdtemp())
    provider = SentenceTransformerProvider("all-MiniLM-L6-v2")
    engine = SemanticSearchEngine(provider=provider, cache_dir=temp_dir)
    
    # Add content with clear relationships
    content_pairs = [
        ("Python programming tutorial for beginners", "tutorial"),
        ("Advanced Python programming concepts", "tutorial"),
        ("Machine learning with Python libraries", "ml"),
        ("Deep learning neural networks", "ml"),
        ("Web development using JavaScript", "web"),
        ("Frontend web development frameworks", "web"),
        ("Database design and optimization", "database"),
        ("SQL queries and data manipulation", "database")
    ]
    
    for content, category in content_pairs:
        await engine.add_content(
            content=content,
            source_id=f"{category}-content",
            source_type=category,
            tags=[category]
        )
        
    print(f"âœ… Added {len(content_pairs)} categorized content items")
    
    # Test different similarity algorithms
    query = "Python machine learning programming"
    algorithms = [
        SimilarityAlgorithm.COSINE,
        SimilarityAlgorithm.DOT_PRODUCT,
        SimilarityAlgorithm.EUCLIDEAN
    ]
    
    print(f"\nğŸ” Testing similarity algorithms for query: '{query}'")
    
    for algorithm in algorithms:
        print(f"\nğŸ“ Using {algorithm.value} similarity:")
        
        # Generate query embedding
        query_vector = await provider.generate_embedding(query)
        import numpy as np
        query_array = np.array(query_vector)
        
        # Search using specific algorithm
        similar_ids = engine.index.search(
            query_array,
            k=5,
            algorithm=algorithm
        )
        
        print(f"   Found {len(similar_ids)} results:")
        for i, (embedding_id, score) in enumerate(similar_ids[:3]):
            if embedding_id in engine.index.embeddings:
                embedding = engine.index.embeddings[embedding_id]
                source_type = embedding.metadata.source_type
                print(f"     {i+1}. Score: {score:.4f} | Type: {source_type} | ID: {embedding_id}")
            else:
                print(f"     {i+1}. Score: {score:.4f} | ID: {embedding_id}")


async def main():
    """Run all semantic search demonstrations."""
    print_header("ENTAERA - Day 4 Kata 4.1: Semantic Search Demonstration")
    print("Demonstrating advanced semantic search with vector embeddings, similarity algorithms,")
    print("conversation integration, and intelligent content retrieval.")
    
    try:
        # Run demonstrations
        await demo_basic_embedding_and_search()
        await demo_similarity_algorithms()
        
        print_header("âœ… Semantic Search Demonstration Completed Successfully!", "=")
        print("âœ… Vector embeddings and similarity search working perfectly")
        print("âœ… Multiple similarity algorithms implemented and tested")
        print("âœ… High-performance search with comprehensive caching")
        print("âœ… Ready for integration with conversation management")
        
    except Exception as e:
        print(f"\nâŒ Demo failed with error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # Run the demonstration
    asyncio.run(main())